{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도학습 알고리즘 (Supervised Learning)\n",
    "학습할 데이터와 명시적인 정답(레이블)을 이용해 데이터의 특성과 분포를 학습 후 예측\n",
    "* K-최근접 이웃 알고리즘 (KNN)\n",
    "* 서포트 벡터 머신 (SVM)\n",
    "> 둘 다 **분류(Classification)** 방식에 속하는 **지도학습** 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-최근접 이웃 알고리즘 (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN 알고리즘의 개념\n",
    "**KNN : K-Nearest Neighbor Algorithm**  \n",
    "* 이웃 : 가까이 존재하는 데이터들\n",
    "* 현재 새 데이터로부터 **가까운 K개의 데이터**를 찾아 **K개의 레이블 중 가장 많이 분류된 값으로 분류**하는 알고리즘\n",
    "* 최근접점 찾기 : **피타고라스의 정리**이용! *(두 점 사이의 거리)* \n",
    "![](k_num.png)  \n",
    "k 값이 클 수록 경계가 스무스해짐 (최적의 K를 잘 찾아주어야 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서포트 벡터 머신 (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM의 개념\n",
    "**SVM : Support Vector Machine**  \n",
    "* 서로 다른 분류 값을 결정하는 경계선(**결정 경계선**)을 결정하는 알고리즘\n",
    "* **결정 경계선(decision boundary)** 을 어떻게 긋느냐에 따라 분류값 달라짐\n",
    "* **벡터(vector)의 의미** : 2차원 공간 상에 나타난 **데이터 포인트 (점)**\n",
    "* **서포트 벡터** : 결정 경계선과 **가장 가까이 맞닿은 데이터 포인트 (점)**\n",
    "* **마진(margin)** : 서포트 벡터와 결정 경계 사이의 거리\n",
    "> 즉, **마진이 최소일 때의 데이터 포인트 (가까운 점들) : 서포트 벡터**!    \n",
    "  \n",
    "![](svm.png)  \n",
    "* **SVM의 목표** : ***마진을 '최대'로 하는 결정 경계를 찾는 것!*** (모든 벡터로부터 멀리~) \n",
    "> 마진이 클수록 알지 못하는 새로운 데이터에 대해 안정적으로 분류할 가능성이 높으므로  \n",
    "* 마진을 최대로 하는 결정 경계는 학습 단계를 통해 발견  \n",
    "> cf. N차원의 데이터 벡터 공간 ==> **결정 경계 : N-1차원!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커널 트릭 (SVM의 기능)\n",
    "* 저차원 벡터 공간 데이터(1차원)를 -> 고차원 벡터 공간(2차원)으로 옮기는 방법  \n",
    "* 저차원에서 쉽게 찾을 수 없는 결정 경계를 찾을 수 있도록 하는 방법\n",
    "![](1d.png)\n",
    "![](2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 최적화 (Cost, Gamma)\n",
    "for. SVM에서 **커널 트릭**을 사용하기 위해 최적 파라미터를 설정해주어야 함   \n",
    "  \n",
    "* **1. 비용(Cost)** : **마진** 조절 변수 (마진을 얼마나 할 것인가?)   \n",
    "      \n",
    "      \n",
    "> [ **마진**, **비용**, **학습시 에러율**의 관계 ]\n",
    "* **비용이 작을수록, 마진 너비가 넓어지고, 학습 시 에러율이 높아짐** (두루뭉실하게 선 그음. 꼬불꼬불 X)\n",
    "* **비용이 클수록, 마진 너비가 좁아지고, 학습 시 에러율은 낮아짐** (꼬불꼬불, 디테일하고 가까운 선)   \n",
    "  ==> ***BUT, '학습되지 않은' 새로운 데이터는 에러가 발생될 가능성이 있음!*** ![](cost.png)  \n",
    "    \n",
    "      \n",
    "* **2. 감마(Gamma)** : 학습 데이터들이 **결정 경계에 영향을 끼치는 범위를 조절**해주는 변수    \n",
    "(뭉쳐있는 정도)\n",
    "  \n",
    "> [ **감마**와 **결정 경계**의 관계 ]  \n",
    "* **감마 값이 클수록, 결정 경계가 작아지고 구부러짐** (많은 데이터 포인트들이 뭉쳐서 가까이 있는 것으로 고려)\n",
    "* **감마 값이 작을수록, 결정 경계가 완만해짐** (데이터 포인트들이 멀리 분포되어 있는 것으로 고려)  \n",
    "![](gamma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서, **비용(Cost)과 감마(Gamma)** 둘 다 **값이 클수록 꼬불꼬불하고 디테일한 결정 경계!**\n",
    "> 두 파라미터를 잘 조정해서 정확도 높이기  \n",
    "  \n",
    "![](cost,gamma.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
